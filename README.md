
## Demo

![Olist demo](assets/videos/Olist-demo.gif)

# Olist Ecommerce Analytics â€” Azure Databricks + Power BI

**End-to-end retail analytics on the public Olist dataset.**  
Ingest & model in **Azure Databricks**, store curated data in **Azure Blob (SAS)**, and deliver insights with **Power BI** (KPI, **RFM segmentation**, drill-through).

> âš ï¸ All secrets are **redacted**. Replace placeholders with your own credentials (Databricks **Secret Scope** or environment variables).

---

## TL;DR

- â­ **Business:** growth levers via **RFM cohorts**, top sellers/customers/categories, YoY trends.  
- ğŸ› ï¸ **Tech:** Notebooks â†’ **star schema** â†’ PBIX; fast visuals; RLS sanity page.  
- â±ï¸ **60-sec demo:** open `powerbi/Olist-Dashboard.pbix`, point to `data/*/*.csv`, **Refresh**.


---


### Dashboards (screenshots)

<p align="center">
  <img src="assets/dashboards/Performance_overview.png" width="45%"/>
  <img src="assets/dashboards/Performance_overview2.png" width="45%"/>
</p>

<p align="center">
  <img src="assets/dashboards/Customer_segmentation%20(RFM).png" width="45%"/>
  <img src="assets/dashboards/drillthrough%201%20seller%20insight.png" width="45%"/>
</p>

<p align="center">
  <img src="assets/dashboards/drillthrough%202%20customers%20performance.png" width="45%"/>
  <img src="assets/dashboards/drillthrough3%20Productcategory%20details.png" width="45%"/>
</p>

<p align="center">
  <img src="assets/dashboards/RLS%20check1.png" width="45%"/>
  <img src="assets/dashboards/RLS%20check2.png" width="45%"/>
</p>




.
â”œâ”€ Databricks/
â”‚  â””â”€ notebooks/
â”‚     â”œâ”€ 00_bootstrap_conf.py         # Spark SAS auth (use Secret Scope in real runs)
â”‚     â””â”€ 01_etl_olist.sql             # SQL notebook: unify Olist & create star views
â”œâ”€ data/                              # curated star-schema CSVs for local demo
â”‚  â”œâ”€ Dim_Customer.csv
â”‚  â”œâ”€ Dim_Date.csv
â”‚  â”œâ”€ Dim_Product.csv
â”‚  â”œâ”€ Dim_Seller.csv
â”‚  â”œâ”€ Seller_State_Emails.csv         # RLS demo mapping
â”‚  â””â”€ Fact_order.csv                  
â”œâ”€ powerbi/
â”‚  â””â”€ Olist-Dashboard.pbix
â”œâ”€ assets/
â”‚  â”œâ”€ azure/                          # redacted infra screenshots (SAS, containers, CSVs)
â”‚  â”œâ”€ dashboards/                     # dashboard page screenshots
â”‚  â”œâ”€ databricks/                     # sanitized notebook screenshot
â”‚  â””â”€ videos/                         # Olist-demo.gif + .mp4 
â”œâ”€ docs/                              # deep dives (model, RFM, measures, security, roadmap)
â”‚  â”œâ”€ engineering.md
â”‚  â”œâ”€ powerbi_model.md
â”‚  â”œâ”€ rfm.md
â”‚  â”œâ”€ measures.md
â”‚  â”œâ”€ security.md
â”‚  â”œâ”€ roadmap.md
â”‚  â””â”€ assumptions.md
â”œâ”€ config.template.json               # example config 
â”œâ”€ .gitignore
â””â”€ LICENSE

---

## Architecture

**Raw (Blob/SAS) â†’ Curated (Star schema in Databricks) â†’ BI (Power BI)**  


---

## Quickstart

### Option A â€” Local (no Azure needed)

1. Clone this repo.  
2. Open **`powerbi/Olist-Dashboard.pbix`**.  
3. If prompted, browse to `data/*/*.csv` and **Refresh**.

### Option B â€” Full pipeline (Azure)

1. Create an Azure Storage Account & container `olist-raw` (HTTPS-only SAS).  
2. Import `databricks/notebooks/etl_olist.py` into Databricks.  
3. Store your SAS in a **Secret Scope** and run the notebook to export the star schema back to Blob.  
4. Open the PBIX and connect to those CSVs (Blob or local copy).

## Azure proofs (redacted)

<p align="center">
  <img src="assets/azure/azure-sas-settings.png" width="45%"/>
  <img src="assets/azure/azure-containers-list.png" width="45%"/>
</p>

<p align="center">
  <img src="assets/azure/azure-csv_data.png" width="45%"/>
</p>



** Data Source:** Public Olist e-commerce dataset (Brazil).  
This repo includes the curated **star-schema CSVs** (FactOrders, DimCustomer, DimProduct, DimSeller, DimDate) for local demo.  
Please check the original dataset license for re-use and attribution guidelines.

- Download PBIX: [`powerbi/Olist-Dashboard.pbix`](powerbi/Olist-Dashboard.pbix)
- Sample data (CSV): [`data/`](data/)


Example (sanitized):

```python
# NOTE: the real token is stored in a Secret Scope in Databricks.
# sas_token = dbutils.secrets.get(scope="olist-secrets", key="adls-sas")
sas_token = "<SAS_AT_RUNTIME>"
account   = "olistdatalake2025"
container = "olist-raw"

spark.conf.set(
    f"fs.azure.sas.{container}.{account}.blob.core.windows.net",
    sas_token
)


